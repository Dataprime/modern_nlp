{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with spaCy\n",
    "\n",
    "- Introduction\n",
    "- Setting up and getting started with spaCy\n",
    "- spaCy workflow\n",
    "- spaCY data model\n",
    "- Processing text\n",
    "- Text syntax and structure\n",
    "- Word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Why spaCy?\n",
    "\n",
    "- Curated algorithms\n",
    "- Robust data model\n",
    "- Memory and computationally efficient\n",
    "- Interoperability and customization\n",
    "\n",
    "SpaCy's structure allows accumulated annotations through NLP pipelines while preserving the original source of information. SpaCy is OO, where objects are created and then mutated and queried to get the work done.\n",
    "\n",
    "SpaCy does tokenization, sentence recognition, part of speech tagging, lemmatization, dependency parsing, and named entity recognition all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and getting started with spaCy\n",
    "1. pip install spacy\n",
    "2. python -m spacy.en.download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second line will download the model data for the english model. This will load the parser, tagger, vocabulary and word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import spacy and load english model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse text into Document object\n",
    "doc = nlp(\"I went to school this morning, but it's Sunday. School is closed! Silly me :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each token is an object with lots of different properties. A property with underscore returns the string representation, while a property without an underscore returns an index (int) into spaCy's vocabulary. These are the  properties of the Token class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'has_repvec',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ancestor_of',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'repvec',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sentiment',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print properties and methods of Token class\n",
    "[prop for prop in dir(doc[0]) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the properties of the Doc class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'has_vector',\n",
       " 'is_parsed',\n",
       " 'is_tagged',\n",
       " 'mem',\n",
       " 'merge',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'read_bytes',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print properties and methods of Document class\n",
    "[prop for prop in dir(doc) if not prop.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy workflow\n",
    "1. Encode the input text in unicode (python 3 does this automatically)\n",
    "2. Initiate a language model by one of two ways:\n",
    "    - calling spacy.load(‘en’) and passing the language id or\n",
    "    - grabbing the module directly by calling spacy.en.english\n",
    "3. The language model, which is a document constructor will then take a unicode string as argument and return a Document object with annotations \n",
    "![](images/spaCy_workflow.png)\n",
    "\n",
    "### NLP Pipeline\n",
    "The language model runs it’s functions over the document to build the annotations. These functions are stored in the model’s pipeline object. The functions include:\n",
    "- POS tagger\n",
    "- Dependency parser\n",
    "- Matcher\n",
    "- Named entity recognizer\n",
    "\n",
    "Each function mutates the document in place and once the language model is instantiated, it can be used over and over to process text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode text as unicode (python 3 does that automatically)\n",
    "text = \"I'm running some python code on my jupyter notebook\"\n",
    "\n",
    "# initialize language model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# create a document\n",
    "document = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.tagger.Tagger object at 0x11bfc75e8>\n",
      "<spacy.pipeline.DependencyParser object at 0x11de0d8b8>\n",
      "<spacy.matcher.Matcher object at 0x153c7b518>\n",
      "<spacy.pipeline.EntityRecognizer object at 0x1535e4d18>\n"
     ]
    }
   ],
   "source": [
    "# print the default pipeline functions\n",
    "for function in nlp.pipeline:\n",
    "    print(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the Language Model\n",
    "\n",
    "The language model is fully customizable. You can add or remove functions from the pipeline. Let's see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP\n",
      "like VBP\n",
      "to TO\n",
      "watch VB\n",
      "starwars NNS\n",
      "movies NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# Run the standard language model\n",
    "starwars_text = 'I like to watch starwars movies.'\n",
    "document = nlp(starwars_text)\n",
    "for token in document:\n",
    "    print(token.orth_, token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.tagger.Tagger object at 0x153f4e318>\n",
      "<spacy.pipeline.DependencyParser object at 0x155da42c8>\n",
      "<spacy.matcher.Matcher object at 0x18bc6fc18>\n",
      "<spacy.pipeline.EntityRecognizer object at 0x18b5bf548>\n",
      "<function identify_starwars at 0x153c852f0>\n"
     ]
    }
   ],
   "source": [
    "# Modify the default language model\n",
    "def identify_starwars(doc):\n",
    "    for token in doc:\n",
    "        if token.text == 'starwars':\n",
    "            token.tag_ = 'NNP'\n",
    "            \n",
    "def get_new_pipeline(nlp):\n",
    "    return [nlp.tagger, nlp.parser, nlp.matcher, nlp.entity, identify_starwars]\n",
    "\n",
    "custom_nlp = spacy.load('en', create_pipeline=get_new_pipeline)\n",
    "new_document = custom_nlp(starwars_text)\n",
    "\n",
    "for function in custom_nlp.pipeline:\n",
    "    print(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP\n",
      "like VBP\n",
      "to TO\n",
      "watch VB\n",
      "starwars NNP\n",
      "movies NNS\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for token in new_document:\n",
    "    print(token.orth_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-threading\n",
    "\n",
    "Natural Language Processing is a task that can usually be paralellized. This means that processing a document is normally independent of processing another document. The function nlp.pipe returns a generator of document objects that can use multiple threads to process an array or generator of texts. Because it returns a generator of document objects, the pipeline is only run once we access the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['This text will be multiplied ten thousand times.'] * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.25 s, sys: 122 ms, total: 2.37 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=100, n_threads=4):\n",
    "    doc.is_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy data model\n",
    "\n",
    "Other NLP libraries like NLTK provide pure functions for tasks like tokenization and POS tagging. Those functions destroy the input and replace it with processed outputs. One key difference is that spaCy takes an Object Oriented approach, where objects are created and then mutated through added metadata and queried to get work done. To prevent data inconsistencies, spaCy stores the data only once and provides different pointers and views to that data.\n",
    "\n",
    "The central repository in spaCy is called the StringStore. The StringStore is a huge list of strings containing words and annotations, like POS. This way all the token contains is instructions on how to retrieve values, with few integers for references. This helps reduce the footprint of keeping a document in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 VERB\n"
     ]
    }
   ],
   "source": [
    "# properties without '_' at the end are indexes to the StringStore\n",
    "doc = nlp(\"Let's take a look at how spaCy's data is organized.\")\n",
    "token = doc[0]\n",
    "print(token.pos, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "# access the same index through the StringStore\n",
    "POS_index = token.pos\n",
    "StringStore = token.vocab.strings\n",
    "\n",
    "value = StringStore[POS_index]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 IS_ALPHA\n",
      "2 IS_ASCII\n",
      "3 IS_DIGIT\n",
      "4 IS_LOWER\n",
      "5 IS_PUNCT\n",
      "6 IS_SPACE\n",
      "82 ADJ\n",
      "83 ADP\n",
      "84 ADV\n",
      "100 EOL\n",
      "101 SPACE\n",
      "102 Animacy_anim\n",
      "200 PronType_ind\n",
      "500 en\n",
      "501 the\n",
      "502 xxx\n",
      "1000 igh\n",
      "1001 Is\n",
      "1002 Bush\n",
      "1003 bush\n",
      "5000 guaranteed\n",
      "7000 July\n",
      "50000 debt-discounting\n"
     ]
    }
   ],
   "source": [
    "# print a piece of the StringStore\n",
    "for i in [1, 2, 3, 4, 5, 6, 82, 83, 84, 100, 101, 102, 200, 500, 501, 502, 1000, 1001, 1002, 1003, 5000, 7000, 50000]:\n",
    "    print(i, StringStore[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summarized representation of spaCy's data model.\n",
    "\n",
    "![](images/spaCy_data_model.png)\n",
    "\n",
    "Many of the atributes stored in a token are context specific, which means that we will need information from the document to determine their values.\n",
    "\n",
    "- *ex: \"hit\" can be a noun or a verb*\n",
    "\n",
    "SpaCy has data structures called *lexemes* to store all non-context specific information about a word, like its lower case form, shape, suffix, etc.\n",
    "\n",
    "Similarly, a document simply points to its tokens. A document also provides spans, which are subsequences of tokens such as multi-token entities and noun chunks.\n",
    "\n",
    "![](images/spaCy_data_model_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizing words, spans and sentences\n",
    "* Removing stopwords\n",
    "* Removing punctuations\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Tokens and Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, brown, fox, is, quick, and, he, is, jumping, over, the, lazy, dog]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The brown fox is quick and he is jumping over the lazy dog\")\n",
    "\n",
    "# print all the tokens\n",
    "print([token for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 13\n",
      "First token: The\n",
      "Last token: dog\n",
      "Tokens 2 through 4: brown fox is\n"
     ]
    }
   ],
   "source": [
    "# another way to do this is indexing the doc\n",
    "print('Number of tokens: {}'.format(len(doc)))\n",
    "print('First token: {}'.format(doc[0]))\n",
    "print('Last token: {}'.format(doc[-1]))\n",
    "print('Tokens 2 through 4: {}'.format(doc[1:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown fox\n",
      "he\n",
      "the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# print all the noun chunks\n",
    "for np in doc.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Sentences\n",
    "\n",
    "- spaCy performs sentence boundary detection to automatically detect sentences\n",
    "- calling document.sents returns an iterable of sentences\n",
    "- because spaCy returns generators, it postpones doing work until we actually need results\n",
    "    - this allows us to process individual elements of an array without having to load everything to memory all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to school this morning, but it's Sunday.\n",
      "School is closed!\n",
      "Silly me :)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I went to school this morning, but it's Sunday. School is closed! Silly me :)\")\n",
    "\n",
    "# print all the sentences\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, to, this, but, it, is, me]\n"
     ]
    }
   ],
   "source": [
    "# print all of the stop words in the document\n",
    "print([token for token in doc if token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[went, school, morning, ,, 's, Sunday, ., School, closed, !, Silly, :)]\n"
     ]
    }
   ],
   "source": [
    "# remove the stop words\n",
    "doc_1 = [token for token in doc if not token.is_stop]\n",
    "print(doc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[,, ., !, :)]\n"
     ]
    }
   ],
   "source": [
    "# print all punctuation in the document\n",
    "print([token for token in doc if token.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[went, school, morning, 's, Sunday, School, closed, Silly]\n"
     ]
    }
   ],
   "source": [
    "doc_2 = [token for token in doc_1 if not token.is_punct]\n",
    "print(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey INTJ\n",
      ", PUNCT\n",
      "this DET\n",
      "is VERB\n",
      "gr8 NOUN\n",
      "! PUNCT\n",
      "I PRON\n",
      "rly ADV\n",
      "enjoy VERB\n",
      "it PRON\n",
      "! PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('Hey, this is gr8! I rly enjoy it!')\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'school', 'morning', 'be', 'sunday', 'school', 'closed', 'silly']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [token.lemma_ for token in doc_2]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Syntax and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Part-of-Speech (POS) tagging\n",
    "* Dependency parsing\n",
    "* Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging\n",
    "* allows you to know what is a verb and what is a noun\n",
    "* can't always rely on a lexicon (ex: hit can be a noun or verb)\n",
    "    * \"You can verb anything.\" William Safire\n",
    "* Can be achieved with dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN\t:\tPTB_TAG\t:\tUNIVERSAL_TAG\n",
      "---------------------------------------------\n",
      "I\t:\tPRP\t:\tPRON\n",
      "went\t:\tVBD\t:\tVERB\n",
      "to\t:\tIN\t:\tADP\n",
      "school\t:\tNN\t:\tNOUN\n",
      "this\t:\tDT\t:\tDET\n",
      "morning\t:\tNN\t:\tNOUN\n",
      ",\t:\t,\t:\tPUNCT\n",
      "but\t:\tCC\t:\tCCONJ\n",
      "it\t:\tPRP\t:\tPRON\n",
      "'s\t:\tVBZ\t:\tVERB\n",
      "Sunday\t:\tNNP\t:\tPROPN\n",
      ".\t:\t.\t:\tPUNCT\n",
      "School\t:\tNNP\t:\tPROPN\n",
      "is\t:\tVBZ\t:\tVERB\n",
      "closed\t:\tJJ\t:\tADJ\n",
      "!\t:\t.\t:\tPUNCT\n",
      "Silly\t:\tVB\t:\tVERB\n",
      "me\t:\tPRP\t:\tPRON\n",
      ":)\t:\t.\t:\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "print('TOKEN\\t:\\tPTB_TAG\\t:\\tUNIVERSAL_TAG')\n",
    "print('---------------------------------------------')\n",
    "for token in doc:\n",
    "    print('{}\\t:\\t{}\\t:\\t{}'.format(token, token.tag_, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing\n",
    "The sentence meaning comes from combining *chunks* of meaning. Think of words as functions, where they have arguments (dependent words).\n",
    "\n",
    "*ex: \"She hit the wall.\"*\n",
    "    - \"hit\" requires a hitter (nsubj) and a hittee (direct object)\n",
    "    - \"hit defines how this sentence flows\"\n",
    "![](images/displacy_example.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEFT <--- WORD[WORD_TYPE] --> RIGHT\n",
      "-----------------------------------\n",
      "[] <--- The[det] ---> []\n",
      "[] <--- brown[amod] ---> []\n",
      "['The', 'brown'] <--- fox[nsubj] ---> []\n",
      "['fox'] <--- is[ROOT] ---> ['quick', 'jumping']\n",
      "[] <--- quick[acomp] ---> ['and']\n",
      "[] <--- and[cc] ---> []\n",
      "[] <--- he[nsubj] ---> []\n",
      "[] <--- is[aux] ---> []\n",
      "['he', 'is'] <--- jumping[ccomp] ---> ['over']\n",
      "[] <--- over[prep] ---> ['dog']\n",
      "[] <--- the[det] ---> []\n",
      "[] <--- lazy[amod] ---> []\n",
      "['the', 'lazy'] <--- dog[pobj] ---> []\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp('The brown fox is quick and he is jumping over the lazy dog')\n",
    "\n",
    "def print_dep_parsing(doc):\n",
    "    header = 'LEFT <--- WORD[WORD_TYPE] --> RIGHT\\n-----------------------------------'\n",
    "    dep_pattern = '{left} <--- {word}[{word_type}] ---> {right}'\n",
    "    print(header)\n",
    "    for token in doc2:\n",
    "        print(dep_pattern.format(\n",
    "            word=token.orth_,\n",
    "            word_type=token.dep_,\n",
    "            left=[t.orth_ for t in token.lefts],\n",
    "            right=[t.orth_ for t in token.rights]\n",
    "        ))\n",
    "\n",
    "print_dep_parsing(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to visualize the dependency parsing is using [displayCy](https://demos.explosion.ai/displacy/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition\n",
    "Named entities can be accessed through doc.ents. Entity recognition allows finding proper nouns, such as specific people and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN : NAMED_ENTITY\n",
      "--------------------\n",
      "New York : GPE\n",
      "morning : TIME\n",
      "Culture Espresso : ORG\n",
      "Susan : PERSON\n",
      "the Central Park : LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I flew to New York in the morning, got breakfast at Culture Espresso and met Susan at the Central Park.')\n",
    "\n",
    "print('TOKEN : NAMED_ENTITY')\n",
    "print('--------------------')\n",
    "for ent in doc.ents:\n",
    "    print('{} : {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors\n",
    "\n",
    "SpaCy makes using word vectors really easy. The English model installs vectors for 1 million vocabulary entries, using 300-dimensional vectors trained on the Common Crawl corpus using the [GloVe](https://nlp.stanford.edu/projects/glove/) algorithm.\n",
    "\n",
    "You can access the vector form a Lexeme, Token, Span or Doc class through the '.vector' property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Math quiz: do you remember your linear algebra classes?')\n",
    "print(doc[0].vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16111     0.37452999  0.032565   -0.040995   -0.0052605   0.15727\n",
      " -0.2818      0.23202001  0.31810999  1.56599998 -0.28884    -0.11199\n",
      "  0.31402999  0.20852    -0.0071106   0.21749     0.24699999  1.51549995\n",
      " -0.50665998  0.069402    0.27742001 -0.35484001  0.214      -0.016425\n",
      "  0.21309    -0.34055001  0.042495    0.094604    0.47981    -0.0035279\n",
      " -0.17168     0.16067     0.64766002 -0.15987     0.37630999  0.29563001\n",
      "  0.24669001  0.64608997 -0.10777    -0.068073    0.25356001 -0.66258001\n",
      " -0.38949999 -0.13439     0.17922001  0.2665      0.12296     0.36366999\n",
      "  0.35499001 -0.17848    -0.49405     0.30045     0.52583998  0.45199999\n",
      " -0.77667999 -0.05694     0.21585     0.19785    -0.094671    0.11303\n",
      " -0.20819999  0.042582   -0.013595   -0.22459     0.46709001  0.077615\n",
      " -0.55409998  0.86278999  0.92205     0.13732    -0.34322    -0.16752\n",
      "  0.088037   -0.60540998 -0.11989     0.73468     0.16678999  0.34481999\n",
      "  0.10886     1.10099995 -0.49709001 -0.38852     0.59628999  0.36003\n",
      "  0.35055     0.22702999  0.017875   -0.015725   -0.28496    -0.022788\n",
      " -0.30803999  0.23480999 -0.44154    -0.27500999  0.12998    -0.39706001\n",
      " -0.24528     0.38457999  0.61097997  0.15429001  0.21512    -0.13405\n",
      " -0.26682001 -0.23475     0.32104    -1.2112      0.34621999 -0.04006\n",
      "  0.029454    0.23638    -0.19372    -0.097339   -0.24845999 -0.49840999\n",
      " -0.14049    -0.040621   -0.39014    -0.20991001 -0.01665    -0.50291997\n",
      "  0.93145001 -0.37226999  0.53349     0.39592001  0.18449999  0.88774002\n",
      " -0.041811    0.49857     0.30454001  0.12155     0.22262     0.20657\n",
      " -0.070209    0.41273001  0.062253   -0.053749    0.49169001  0.093869\n",
      " -0.25984001  0.011134   -1.13590002  0.071517   -0.21326999  0.049028\n",
      " -0.093004   -0.2723      0.084233    0.045197   -0.30548999 -0.14204\n",
      "  0.70411003 -0.12433    -0.40195     0.55091     0.26027    -0.37810001\n",
      "  0.27919    -0.34968999  0.059945   -0.70394999  0.097523    0.15287\n",
      " -0.16783001 -0.43854001 -0.32607999  0.15875    -0.58515    -0.41689\n",
      "  0.12017    -0.044443    0.095198   -0.14736     0.76537001 -0.57827002\n",
      " -0.010428   -0.56080002 -0.35992    -0.24218     0.089512   -0.46351001\n",
      "  0.28365001  0.22898    -0.14112     0.59780002 -0.15535     0.1416\n",
      " -0.4707      0.061915    0.041303   -0.021825   -0.02273    -0.26137\n",
      " -0.77340001 -0.14257     0.19687     0.061187    0.32080001 -0.047045\n",
      "  0.11112    -0.13685     0.078784    0.1901      0.13617     0.66465002\n",
      " -0.2719     -0.58692002 -0.57832003 -0.014283    0.50045002  0.04565\n",
      " -0.38929     0.46037     0.054409   -0.51697999 -0.23657    -0.69255\n",
      " -0.43562001  0.16728    -0.29337001  0.73952001  0.11609     0.032757\n",
      " -0.52025998 -0.56013     0.44022     0.79469001  0.0097085  -0.0064776\n",
      "  0.16587    -0.11696    -0.30379    -0.091559    0.28588    -0.11017\n",
      " -0.83346999 -0.076193    0.34979999  0.50032002 -0.10075     0.21923999\n",
      "  0.16218001 -0.80447    -0.54588002  0.24977    -0.19503    -0.41409999\n",
      " -0.10275    -0.57258999  0.28784001  0.026841    0.1163      0.31922999\n",
      " -0.21416    -0.25163001  0.59107    -0.13335    -0.34161001  0.067109\n",
      "  0.43333    -0.13258     0.42085001  0.40779001 -0.63414001 -0.13558\n",
      "  0.25341001 -0.52603    -0.84116     0.10729     0.89524001  0.037148\n",
      " -1.11450005 -0.0063449  -0.19812    -0.35236999 -0.95258999 -0.91404003\n",
      " -0.32460001 -0.32034999 -0.64705002 -0.59837002 -0.18968999  0.078866\n",
      " -0.18684     0.18162     0.083233   -0.72407001 -0.0053721  -0.36493\n",
      " -0.67720002  0.41284001  0.65934998 -0.29124999  0.086124    0.60136002\n",
      "  0.23259    -0.070926    0.075515   -0.066808    0.3107     -0.11629   ]\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity\n",
    "\n",
    "SpaCy has also a convenient way of calculating cosine similarity between words and documents. All we have to do is access the *.similarity* method from the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes\n"
     ]
    }
   ],
   "source": [
    "# get a few words and spans form the last utterance\n",
    "linear_algebra = doc[-4:-2]\n",
    "classes = doc[-2]\n",
    "math = doc[0]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35822012113274243"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.similarity(linear_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the similarity between words from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words fom vocabulary\n",
    "happy = nlp.vocab['happy']\n",
    "excited = nlp.vocab['excited']\n",
    "sad = nlp.vocab['sad']\n",
    "green = nlp.vocab['green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31557633403397889"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare similarity\n",
    "happy.similarity(green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar_words(word):\n",
    "    \n",
    "    # get all the words form the corpus, only lower case\n",
    "    all_words = [w for w in nlp.vocab if w.has_vector \\\n",
    "                 and w.orth_.islower() and w.lower_ != word.lower_]\n",
    "    \n",
    "    # sort the words based on similarity\n",
    "    sorted_list = sorted(all_words, key=word.similarity, reverse=True)\n",
    "    \n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasta\n",
      "burger\n",
      "sandwiches\n",
      "sandwich\n",
      "burgers\n",
      "cheese\n",
      "bread\n",
      "steak\n",
      "fries\n",
      "salad\n"
     ]
    }
   ],
   "source": [
    "car = nlp.vocab['pizza']\n",
    "for word in get_similar_words(car)[:10]:\n",
    "    print(word.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
